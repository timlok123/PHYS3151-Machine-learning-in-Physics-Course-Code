{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2EOr4KKLkL"
      },
      "source": [
        "Prepare the function necessary for building neural network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0dILPeaekO7V"
      },
      "outputs": [],
      "source": [
        "# Initialize a network\n",
        "# n_inputs: the number of inputs\n",
        "# n_hidden_1: the number of neurons in 1st hidden layer, each neuron has n_inputs+1 weights\n",
        "# n_hidden_2: the number of neurons in 2nd hidden layer, each neuron has n_hidden_1+1 weights\n",
        "\n",
        "from random import seed\n",
        "from random import random\n",
        "from random import randrange\n",
        "\n",
        "# n_outputs: the number of outputs, each neuron has n_hidden+1 weights\n",
        "# network contains all the weight, has 3 layers \n",
        "# network[0] has the structure [[], [], []] (cause next layer has 3 neurons)\n",
        "# network[0][0] contains [theta_{11}^(1),theta_{12}^(1), ... ,theta_{18}^(1), theta_{10}^(1) (weight for bias)]\n",
        "# Outlayer has 2 neurons, if index 1 is larger, then takes 1 as output \n",
        "def initialize_network(n_inputs, n_hidden_1, n_hidden_2, n_outputs):\n",
        "    # As \"weight\": [s^{l+1} times s^{l} + 1]\n",
        "    network = list()\n",
        "    hidden_layer_1 = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden_1)]\n",
        "    network.append(hidden_layer_1)\n",
        "    hidden_layer_2 = [{'weights':[random() for i in range(n_hidden_1 + 1)]} for i in range(n_hidden_2)]\n",
        "    network.append(hidden_layer_2)\n",
        "    output_layer = [{'weights':[random() for i in range(n_hidden_2 +1)]} for i in range(n_outputs)]\n",
        "    network.append(output_layer)\n",
        "    return network\n",
        "\n",
        "from math import exp\n",
        "# Calculate neuron activation for an input\n",
        "# Finding a^1(2) ... in lecture slide P.6 \n",
        "def activate(weights, inputs):\n",
        "    #This taking the last value of weight as bias\n",
        "    activation = weights[-1]*1\n",
        "    for i in range(len(weights)-1):\n",
        "        activation += weights[i] * inputs[i]\n",
        "    return activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "# This is the sigmoid function\n",
        "def transfer(activation):\n",
        "    return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "# row has a structure [..., ... ,...]\n",
        "# inputs is the simple list contains [a_1^(2), a_2^(2), ...] (the activation value of each layer)\n",
        "def forward_propagate(network, row):\n",
        "    inputs = row\n",
        "    for layer in network:\n",
        "        new_inputs = []\n",
        "        # The following for-loop is calculating a_1^(2), a_2^(2), ...\n",
        "        # And the result is put in the dictionaries, with \"output\" as tag\n",
        "        # It has the structure {}\n",
        "        for neuron in layer:\n",
        "            activation = activate(neuron['weights'], inputs)\n",
        "            neuron['output'] = transfer(activation)\n",
        "            new_inputs.append(neuron['output'])\n",
        "        inputs = new_inputs\n",
        "    return inputs\n",
        "\n",
        "def transfer_derivative(output):\n",
        "    return output * (1.0 - output)\n",
        "\n",
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "    for i in reversed(range(len(network))):\n",
        "        layer = network[i]\n",
        "        errors = list()\n",
        "        # When it is not the last layer of the structure \n",
        "        if i != len(network)-1:\n",
        "            for j in range(len(layer)):\n",
        "                error = 0.0\n",
        "                for neuron in network[i + 1]:\n",
        "                    error += (neuron['weights'][j] * neuron['delta'])\n",
        "                errors.append(error)\n",
        "        else:\n",
        "            for j in range(len(layer)):\n",
        "                neuron = layer[j]\n",
        "                errors.append(expected[j] - neuron['output'])\n",
        "\n",
        "        # Mark the difference as delta     \n",
        "        for j in range(len(layer)):\n",
        "            neuron = layer[j]\n",
        "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        "\n",
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "    for i in range(len(network)):\n",
        "        inputs = row[:-1]\n",
        "        if i != 0:\n",
        "            inputs = [neuron['output'] for neuron in network[i-1]]\n",
        "        for neuron in network[i]:\n",
        "            for j in range(len(inputs)):\n",
        "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
        "# Training has 3 main steps\n",
        "# 1. Forward propagate\n",
        "# 2. Backward propagate and get all delta \n",
        "# 3. Update the weight and repeats the steps for every output \n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "    for epoch in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            outputs = forward_propagate(network, row)\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "            expected[int(row[-1])] = 1\n",
        "            # This is the a^4 - y \n",
        "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "            backward_propagate_error(network, expected)\n",
        "            update_weights(network, row, l_rate)\n",
        "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\n",
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "    outputs = forward_propagate(network, row)\n",
        "    return outputs.index(max(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0084E3lKDqY"
      },
      "source": [
        "The following part is doing some feature scaling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZabhmU6NlH8E"
      },
      "outputs": [],
      "source": [
        "# Feature scaling \n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "    minmax = list()\n",
        "    stats = [[min(column),max(column)] for column in zip(*dataset)]\n",
        "    return stats\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "    for row in dataset:\n",
        "        for i in range(len(row)-1):\n",
        "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rRNcbhwKVjU"
      },
      "source": [
        "The following part is preparing cross validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w8zyYiC3lNs2"
      },
      "outputs": [],
      "source": [
        "# Split a dataset into k folds\n",
        "# k-fold cross-validation with 5 folds. 769/5=153.8 or 153 records in each fold.\n",
        "def cross_validation_split(dataset, n_folds=5):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct/float(len(actual)) * 100.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXcVrFDhKiZJ"
      },
      "source": [
        "After all the preaparation, we can make use of all the functions and train the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aydY5Z5LKpVj"
      },
      "outputs": [],
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "\n",
        "        predicted, network = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return (scores, network)\n",
        "\n",
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden_1, n_nidden_2):\n",
        "    #the train structure is like [[... ,... ,... ,0], [... ,... ,... ,1], [... ,... ,... ,0]...]\n",
        "    # n_inputs - take the 1st list, then ignore the last term\n",
        "    # n_outputs - either 0 or 1, so it is equal to 2 \n",
        "    n_inputs = len(train[0]) - 1\n",
        "    n_outputs = len(set([row[-1] for row in train]))\n",
        "\n",
        "    # Add terms \"n_hidden_2\" here\n",
        "    network = initialize_network(n_inputs, n_hidden_1, n_hidden_2, n_outputs)\n",
        "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        prediction = predict(network, row)\n",
        "        predictions.append(prediction)\n",
        "    print(n_inputs)\n",
        "    print(n_outputs)\n",
        "    return(predictions, network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjWCUXp0lyFT",
        "outputId": "f9d423cc-92dd-43a0-cee0-1c99d0e6b721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">epoch=0, lrate=0.100, error=301.319\n",
            ">epoch=1, lrate=0.100, error=281.014\n",
            ">epoch=2, lrate=0.100, error=280.962\n",
            ">epoch=3, lrate=0.100, error=280.909\n",
            ">epoch=4, lrate=0.100, error=280.855\n",
            ">epoch=5, lrate=0.100, error=280.800\n",
            ">epoch=6, lrate=0.100, error=280.744\n",
            ">epoch=7, lrate=0.100, error=280.686\n",
            ">epoch=8, lrate=0.100, error=280.627\n",
            ">epoch=9, lrate=0.100, error=280.565\n",
            ">epoch=10, lrate=0.100, error=280.502\n",
            ">epoch=11, lrate=0.100, error=280.436\n",
            ">epoch=12, lrate=0.100, error=280.368\n",
            ">epoch=13, lrate=0.100, error=280.297\n",
            ">epoch=14, lrate=0.100, error=280.224\n",
            ">epoch=15, lrate=0.100, error=280.147\n",
            ">epoch=16, lrate=0.100, error=280.067\n",
            ">epoch=17, lrate=0.100, error=279.984\n",
            ">epoch=18, lrate=0.100, error=279.896\n",
            ">epoch=19, lrate=0.100, error=279.804\n",
            ">epoch=20, lrate=0.100, error=279.707\n",
            ">epoch=21, lrate=0.100, error=279.605\n",
            ">epoch=22, lrate=0.100, error=279.497\n",
            ">epoch=23, lrate=0.100, error=279.382\n",
            ">epoch=24, lrate=0.100, error=279.259\n",
            ">epoch=25, lrate=0.100, error=279.128\n",
            ">epoch=26, lrate=0.100, error=278.985\n",
            ">epoch=27, lrate=0.100, error=278.831\n",
            ">epoch=28, lrate=0.100, error=278.663\n",
            ">epoch=29, lrate=0.100, error=278.478\n",
            ">epoch=30, lrate=0.100, error=278.272\n",
            ">epoch=31, lrate=0.100, error=278.043\n",
            ">epoch=32, lrate=0.100, error=277.784\n",
            ">epoch=33, lrate=0.100, error=277.490\n",
            ">epoch=34, lrate=0.100, error=277.152\n",
            ">epoch=35, lrate=0.100, error=276.760\n",
            ">epoch=36, lrate=0.100, error=276.301\n",
            ">epoch=37, lrate=0.100, error=275.759\n",
            ">epoch=38, lrate=0.100, error=275.112\n",
            ">epoch=39, lrate=0.100, error=274.331\n",
            ">epoch=40, lrate=0.100, error=273.378\n",
            ">epoch=41, lrate=0.100, error=272.201\n",
            ">epoch=42, lrate=0.100, error=270.733\n",
            ">epoch=43, lrate=0.100, error=268.882\n",
            ">epoch=44, lrate=0.100, error=266.534\n",
            ">epoch=45, lrate=0.100, error=263.551\n",
            ">epoch=46, lrate=0.100, error=259.795\n",
            ">epoch=47, lrate=0.100, error=255.166\n",
            ">epoch=48, lrate=0.100, error=249.684\n",
            ">epoch=49, lrate=0.100, error=243.563\n",
            ">epoch=50, lrate=0.100, error=237.205\n",
            ">epoch=51, lrate=0.100, error=231.066\n",
            ">epoch=52, lrate=0.100, error=225.485\n",
            ">epoch=53, lrate=0.100, error=220.610\n",
            ">epoch=54, lrate=0.100, error=216.441\n",
            ">epoch=55, lrate=0.100, error=212.912\n",
            ">epoch=56, lrate=0.100, error=209.934\n",
            ">epoch=57, lrate=0.100, error=207.423\n",
            ">epoch=58, lrate=0.100, error=205.305\n",
            ">epoch=59, lrate=0.100, error=203.517\n",
            ">epoch=60, lrate=0.100, error=202.005\n",
            ">epoch=61, lrate=0.100, error=200.726\n",
            ">epoch=62, lrate=0.100, error=199.641\n",
            ">epoch=63, lrate=0.100, error=198.718\n",
            ">epoch=64, lrate=0.100, error=197.931\n",
            ">epoch=65, lrate=0.100, error=197.258\n",
            ">epoch=66, lrate=0.100, error=196.681\n",
            ">epoch=67, lrate=0.100, error=196.186\n",
            ">epoch=68, lrate=0.100, error=195.758\n",
            ">epoch=69, lrate=0.100, error=195.388\n",
            ">epoch=70, lrate=0.100, error=195.068\n",
            ">epoch=71, lrate=0.100, error=194.788\n",
            ">epoch=72, lrate=0.100, error=194.544\n",
            ">epoch=73, lrate=0.100, error=194.330\n",
            ">epoch=74, lrate=0.100, error=194.140\n",
            ">epoch=75, lrate=0.100, error=193.972\n",
            ">epoch=76, lrate=0.100, error=193.821\n",
            ">epoch=77, lrate=0.100, error=193.685\n",
            ">epoch=78, lrate=0.100, error=193.561\n",
            ">epoch=79, lrate=0.100, error=193.448\n",
            ">epoch=80, lrate=0.100, error=193.344\n",
            ">epoch=81, lrate=0.100, error=193.247\n",
            ">epoch=82, lrate=0.100, error=193.156\n",
            ">epoch=83, lrate=0.100, error=193.071\n",
            ">epoch=84, lrate=0.100, error=192.991\n",
            ">epoch=85, lrate=0.100, error=192.914\n",
            ">epoch=86, lrate=0.100, error=192.840\n",
            ">epoch=87, lrate=0.100, error=192.769\n",
            ">epoch=88, lrate=0.100, error=192.701\n",
            ">epoch=89, lrate=0.100, error=192.635\n",
            ">epoch=90, lrate=0.100, error=192.571\n",
            ">epoch=91, lrate=0.100, error=192.508\n",
            ">epoch=92, lrate=0.100, error=192.447\n",
            ">epoch=93, lrate=0.100, error=192.387\n",
            ">epoch=94, lrate=0.100, error=192.328\n",
            ">epoch=95, lrate=0.100, error=192.270\n",
            ">epoch=96, lrate=0.100, error=192.213\n",
            ">epoch=97, lrate=0.100, error=192.156\n",
            ">epoch=98, lrate=0.100, error=192.100\n",
            ">epoch=99, lrate=0.100, error=192.045\n",
            "8\n",
            "2\n",
            ">epoch=0, lrate=0.100, error=303.943\n",
            ">epoch=1, lrate=0.100, error=282.088\n",
            ">epoch=2, lrate=0.100, error=282.023\n",
            ">epoch=3, lrate=0.100, error=281.957\n",
            ">epoch=4, lrate=0.100, error=281.889\n",
            ">epoch=5, lrate=0.100, error=281.821\n",
            ">epoch=6, lrate=0.100, error=281.751\n",
            ">epoch=7, lrate=0.100, error=281.679\n",
            ">epoch=8, lrate=0.100, error=281.606\n",
            ">epoch=9, lrate=0.100, error=281.531\n",
            ">epoch=10, lrate=0.100, error=281.454\n",
            ">epoch=11, lrate=0.100, error=281.375\n",
            ">epoch=12, lrate=0.100, error=281.294\n",
            ">epoch=13, lrate=0.100, error=281.210\n",
            ">epoch=14, lrate=0.100, error=281.125\n",
            ">epoch=15, lrate=0.100, error=281.037\n",
            ">epoch=16, lrate=0.100, error=280.946\n",
            ">epoch=17, lrate=0.100, error=280.853\n",
            ">epoch=18, lrate=0.100, error=280.757\n",
            ">epoch=19, lrate=0.100, error=280.658\n",
            ">epoch=20, lrate=0.100, error=280.555\n",
            ">epoch=21, lrate=0.100, error=280.448\n",
            ">epoch=22, lrate=0.100, error=280.338\n",
            ">epoch=23, lrate=0.100, error=280.223\n",
            ">epoch=24, lrate=0.100, error=280.103\n",
            ">epoch=25, lrate=0.100, error=279.976\n",
            ">epoch=26, lrate=0.100, error=279.844\n",
            ">epoch=27, lrate=0.100, error=279.703\n",
            ">epoch=28, lrate=0.100, error=279.553\n",
            ">epoch=29, lrate=0.100, error=279.393\n",
            ">epoch=30, lrate=0.100, error=279.221\n",
            ">epoch=31, lrate=0.100, error=279.033\n",
            ">epoch=32, lrate=0.100, error=278.828\n",
            ">epoch=33, lrate=0.100, error=278.602\n",
            ">epoch=34, lrate=0.100, error=278.349\n",
            ">epoch=35, lrate=0.100, error=278.064\n",
            ">epoch=36, lrate=0.100, error=277.738\n",
            ">epoch=37, lrate=0.100, error=277.362\n",
            ">epoch=38, lrate=0.100, error=276.922\n",
            ">epoch=39, lrate=0.100, error=276.401\n",
            ">epoch=40, lrate=0.100, error=275.775\n",
            ">epoch=41, lrate=0.100, error=275.013\n",
            ">epoch=42, lrate=0.100, error=274.076\n",
            ">epoch=43, lrate=0.100, error=272.911\n",
            ">epoch=44, lrate=0.100, error=271.449\n",
            ">epoch=45, lrate=0.100, error=269.606\n",
            ">epoch=46, lrate=0.100, error=267.277\n",
            ">epoch=47, lrate=0.100, error=264.350\n",
            ">epoch=48, lrate=0.100, error=260.720\n",
            ">epoch=49, lrate=0.100, error=256.337\n",
            ">epoch=50, lrate=0.100, error=251.260\n",
            ">epoch=51, lrate=0.100, error=245.697\n",
            ">epoch=52, lrate=0.100, error=239.973\n",
            ">epoch=53, lrate=0.100, error=234.425\n",
            ">epoch=54, lrate=0.100, error=229.288\n",
            ">epoch=55, lrate=0.100, error=224.665\n",
            ">epoch=56, lrate=0.100, error=220.564\n",
            ">epoch=57, lrate=0.100, error=216.957\n",
            ">epoch=58, lrate=0.100, error=213.804\n",
            ">epoch=59, lrate=0.100, error=211.071\n",
            ">epoch=60, lrate=0.100, error=208.724\n",
            ">epoch=61, lrate=0.100, error=206.733\n",
            ">epoch=62, lrate=0.100, error=205.066\n",
            ">epoch=63, lrate=0.100, error=203.683\n",
            ">epoch=64, lrate=0.100, error=202.544\n",
            ">epoch=65, lrate=0.100, error=201.605\n",
            ">epoch=66, lrate=0.100, error=200.830\n",
            ">epoch=67, lrate=0.100, error=200.187\n",
            ">epoch=68, lrate=0.100, error=199.650\n",
            ">epoch=69, lrate=0.100, error=199.198\n",
            ">epoch=70, lrate=0.100, error=198.814\n",
            ">epoch=71, lrate=0.100, error=198.487\n",
            ">epoch=72, lrate=0.100, error=198.204\n",
            ">epoch=73, lrate=0.100, error=197.958\n",
            ">epoch=74, lrate=0.100, error=197.742\n",
            ">epoch=75, lrate=0.100, error=197.549\n",
            ">epoch=76, lrate=0.100, error=197.376\n",
            ">epoch=77, lrate=0.100, error=197.220\n",
            ">epoch=78, lrate=0.100, error=197.076\n",
            ">epoch=79, lrate=0.100, error=196.943\n",
            ">epoch=80, lrate=0.100, error=196.818\n",
            ">epoch=81, lrate=0.100, error=196.701\n",
            ">epoch=82, lrate=0.100, error=196.589\n",
            ">epoch=83, lrate=0.100, error=196.483\n",
            ">epoch=84, lrate=0.100, error=196.381\n",
            ">epoch=85, lrate=0.100, error=196.282\n",
            ">epoch=86, lrate=0.100, error=196.186\n",
            ">epoch=87, lrate=0.100, error=196.093\n",
            ">epoch=88, lrate=0.100, error=196.003\n",
            ">epoch=89, lrate=0.100, error=195.914\n",
            ">epoch=90, lrate=0.100, error=195.827\n",
            ">epoch=91, lrate=0.100, error=195.742\n",
            ">epoch=92, lrate=0.100, error=195.658\n",
            ">epoch=93, lrate=0.100, error=195.575\n",
            ">epoch=94, lrate=0.100, error=195.494\n",
            ">epoch=95, lrate=0.100, error=195.413\n",
            ">epoch=96, lrate=0.100, error=195.334\n",
            ">epoch=97, lrate=0.100, error=195.256\n",
            ">epoch=98, lrate=0.100, error=195.178\n",
            ">epoch=99, lrate=0.100, error=195.102\n",
            "8\n",
            "2\n",
            ">epoch=0, lrate=0.100, error=305.061\n",
            ">epoch=1, lrate=0.100, error=281.897\n",
            ">epoch=2, lrate=0.100, error=281.804\n",
            ">epoch=3, lrate=0.100, error=281.710\n",
            ">epoch=4, lrate=0.100, error=281.615\n",
            ">epoch=5, lrate=0.100, error=281.519\n",
            ">epoch=6, lrate=0.100, error=281.422\n",
            ">epoch=7, lrate=0.100, error=281.323\n",
            ">epoch=8, lrate=0.100, error=281.222\n",
            ">epoch=9, lrate=0.100, error=281.120\n",
            ">epoch=10, lrate=0.100, error=281.014\n",
            ">epoch=11, lrate=0.100, error=280.906\n",
            ">epoch=12, lrate=0.100, error=280.795\n",
            ">epoch=13, lrate=0.100, error=280.679\n",
            ">epoch=14, lrate=0.100, error=280.560\n",
            ">epoch=15, lrate=0.100, error=280.435\n",
            ">epoch=16, lrate=0.100, error=280.304\n",
            ">epoch=17, lrate=0.100, error=280.167\n",
            ">epoch=18, lrate=0.100, error=280.021\n",
            ">epoch=19, lrate=0.100, error=279.866\n",
            ">epoch=20, lrate=0.100, error=279.700\n",
            ">epoch=21, lrate=0.100, error=279.521\n",
            ">epoch=22, lrate=0.100, error=279.326\n",
            ">epoch=23, lrate=0.100, error=279.111\n",
            ">epoch=24, lrate=0.100, error=278.872\n",
            ">epoch=25, lrate=0.100, error=278.603\n",
            ">epoch=26, lrate=0.100, error=278.297\n",
            ">epoch=27, lrate=0.100, error=277.943\n",
            ">epoch=28, lrate=0.100, error=277.527\n",
            ">epoch=29, lrate=0.100, error=277.033\n",
            ">epoch=30, lrate=0.100, error=276.436\n",
            ">epoch=31, lrate=0.100, error=275.705\n",
            ">epoch=32, lrate=0.100, error=274.802\n",
            ">epoch=33, lrate=0.100, error=273.675\n",
            ">epoch=34, lrate=0.100, error=272.259\n",
            ">epoch=35, lrate=0.100, error=270.469\n",
            ">epoch=36, lrate=0.100, error=268.198\n",
            ">epoch=37, lrate=0.100, error=265.308\n",
            ">epoch=38, lrate=0.100, error=261.657\n",
            ">epoch=39, lrate=0.100, error=257.131\n",
            ">epoch=40, lrate=0.100, error=251.727\n",
            ">epoch=41, lrate=0.100, error=245.621\n",
            ">epoch=42, lrate=0.100, error=239.179\n",
            ">epoch=43, lrate=0.100, error=232.849\n",
            ">epoch=44, lrate=0.100, error=227.005\n",
            ">epoch=45, lrate=0.100, error=221.860\n",
            ">epoch=46, lrate=0.100, error=217.472\n",
            ">epoch=47, lrate=0.100, error=213.810\n",
            ">epoch=48, lrate=0.100, error=210.800\n",
            ">epoch=49, lrate=0.100, error=208.355\n",
            ">epoch=50, lrate=0.100, error=206.387\n",
            ">epoch=51, lrate=0.100, error=204.813\n",
            ">epoch=52, lrate=0.100, error=203.557\n",
            ">epoch=53, lrate=0.100, error=202.551\n",
            ">epoch=54, lrate=0.100, error=201.738\n",
            ">epoch=55, lrate=0.100, error=201.073\n",
            ">epoch=56, lrate=0.100, error=200.521\n",
            ">epoch=57, lrate=0.100, error=200.056\n",
            ">epoch=58, lrate=0.100, error=199.660\n",
            ">epoch=59, lrate=0.100, error=199.319\n",
            ">epoch=60, lrate=0.100, error=199.021\n",
            ">epoch=61, lrate=0.100, error=198.760\n",
            ">epoch=62, lrate=0.100, error=198.529\n",
            ">epoch=63, lrate=0.100, error=198.322\n",
            ">epoch=64, lrate=0.100, error=198.137\n",
            ">epoch=65, lrate=0.100, error=197.969\n",
            ">epoch=66, lrate=0.100, error=197.818\n",
            ">epoch=67, lrate=0.100, error=197.679\n",
            ">epoch=68, lrate=0.100, error=197.552\n",
            ">epoch=69, lrate=0.100, error=197.435\n",
            ">epoch=70, lrate=0.100, error=197.326\n",
            ">epoch=71, lrate=0.100, error=197.226\n",
            ">epoch=72, lrate=0.100, error=197.131\n",
            ">epoch=73, lrate=0.100, error=197.043\n",
            ">epoch=74, lrate=0.100, error=196.960\n",
            ">epoch=75, lrate=0.100, error=196.882\n",
            ">epoch=76, lrate=0.100, error=196.808\n",
            ">epoch=77, lrate=0.100, error=196.738\n",
            ">epoch=78, lrate=0.100, error=196.671\n",
            ">epoch=79, lrate=0.100, error=196.607\n",
            ">epoch=80, lrate=0.100, error=196.545\n",
            ">epoch=81, lrate=0.100, error=196.486\n",
            ">epoch=82, lrate=0.100, error=196.430\n",
            ">epoch=83, lrate=0.100, error=196.375\n",
            ">epoch=84, lrate=0.100, error=196.322\n",
            ">epoch=85, lrate=0.100, error=196.271\n",
            ">epoch=86, lrate=0.100, error=196.222\n",
            ">epoch=87, lrate=0.100, error=196.173\n",
            ">epoch=88, lrate=0.100, error=196.127\n",
            ">epoch=89, lrate=0.100, error=196.081\n",
            ">epoch=90, lrate=0.100, error=196.037\n",
            ">epoch=91, lrate=0.100, error=195.993\n",
            ">epoch=92, lrate=0.100, error=195.951\n",
            ">epoch=93, lrate=0.100, error=195.909\n",
            ">epoch=94, lrate=0.100, error=195.868\n",
            ">epoch=95, lrate=0.100, error=195.828\n",
            ">epoch=96, lrate=0.100, error=195.789\n",
            ">epoch=97, lrate=0.100, error=195.751\n",
            ">epoch=98, lrate=0.100, error=195.713\n",
            ">epoch=99, lrate=0.100, error=195.676\n",
            "8\n",
            "2\n",
            ">epoch=0, lrate=0.100, error=297.664\n",
            ">epoch=1, lrate=0.100, error=281.581\n",
            ">epoch=2, lrate=0.100, error=281.528\n",
            ">epoch=3, lrate=0.100, error=281.476\n",
            ">epoch=4, lrate=0.100, error=281.422\n",
            ">epoch=5, lrate=0.100, error=281.367\n",
            ">epoch=6, lrate=0.100, error=281.312\n",
            ">epoch=7, lrate=0.100, error=281.256\n",
            ">epoch=8, lrate=0.100, error=281.199\n",
            ">epoch=9, lrate=0.100, error=281.141\n",
            ">epoch=10, lrate=0.100, error=281.082\n",
            ">epoch=11, lrate=0.100, error=281.023\n",
            ">epoch=12, lrate=0.100, error=280.963\n",
            ">epoch=13, lrate=0.100, error=280.902\n",
            ">epoch=14, lrate=0.100, error=280.840\n",
            ">epoch=15, lrate=0.100, error=280.777\n",
            ">epoch=16, lrate=0.100, error=280.713\n",
            ">epoch=17, lrate=0.100, error=280.649\n",
            ">epoch=18, lrate=0.100, error=280.583\n",
            ">epoch=19, lrate=0.100, error=280.516\n",
            ">epoch=20, lrate=0.100, error=280.448\n",
            ">epoch=21, lrate=0.100, error=280.378\n",
            ">epoch=22, lrate=0.100, error=280.307\n",
            ">epoch=23, lrate=0.100, error=280.234\n",
            ">epoch=24, lrate=0.100, error=280.160\n",
            ">epoch=25, lrate=0.100, error=280.083\n",
            ">epoch=26, lrate=0.100, error=280.003\n",
            ">epoch=27, lrate=0.100, error=279.921\n",
            ">epoch=28, lrate=0.100, error=279.835\n",
            ">epoch=29, lrate=0.100, error=279.746\n",
            ">epoch=30, lrate=0.100, error=279.653\n",
            ">epoch=31, lrate=0.100, error=279.554\n",
            ">epoch=32, lrate=0.100, error=279.450\n",
            ">epoch=33, lrate=0.100, error=279.338\n",
            ">epoch=34, lrate=0.100, error=279.219\n",
            ">epoch=35, lrate=0.100, error=279.089\n",
            ">epoch=36, lrate=0.100, error=278.947\n",
            ">epoch=37, lrate=0.100, error=278.791\n",
            ">epoch=38, lrate=0.100, error=278.617\n",
            ">epoch=39, lrate=0.100, error=278.420\n",
            ">epoch=40, lrate=0.100, error=278.196\n",
            ">epoch=41, lrate=0.100, error=277.937\n",
            ">epoch=42, lrate=0.100, error=277.634\n",
            ">epoch=43, lrate=0.100, error=277.275\n",
            ">epoch=44, lrate=0.100, error=276.842\n",
            ">epoch=45, lrate=0.100, error=276.314\n",
            ">epoch=46, lrate=0.100, error=275.661\n",
            ">epoch=47, lrate=0.100, error=274.840\n",
            ">epoch=48, lrate=0.100, error=273.796\n",
            ">epoch=49, lrate=0.100, error=272.452\n",
            ">epoch=50, lrate=0.100, error=270.709\n",
            ">epoch=51, lrate=0.100, error=268.434\n",
            ">epoch=52, lrate=0.100, error=265.472\n",
            ">epoch=53, lrate=0.100, error=261.661\n",
            ">epoch=54, lrate=0.100, error=256.886\n",
            ">epoch=55, lrate=0.100, error=251.167\n",
            ">epoch=56, lrate=0.100, error=244.734\n",
            ">epoch=57, lrate=0.100, error=238.016\n",
            ">epoch=58, lrate=0.100, error=231.490\n",
            ">epoch=59, lrate=0.100, error=225.511\n",
            ">epoch=60, lrate=0.100, error=220.256\n",
            ">epoch=61, lrate=0.100, error=215.767\n",
            ">epoch=62, lrate=0.100, error=212.016\n",
            ">epoch=63, lrate=0.100, error=208.937\n",
            ">epoch=64, lrate=0.100, error=206.445\n",
            ">epoch=65, lrate=0.100, error=204.448\n",
            ">epoch=66, lrate=0.100, error=202.854\n",
            ">epoch=67, lrate=0.100, error=201.577\n",
            ">epoch=68, lrate=0.100, error=200.546\n",
            ">epoch=69, lrate=0.100, error=199.701\n",
            ">epoch=70, lrate=0.100, error=198.996\n",
            ">epoch=71, lrate=0.100, error=198.399\n",
            ">epoch=72, lrate=0.100, error=197.885\n",
            ">epoch=73, lrate=0.100, error=197.436\n",
            ">epoch=74, lrate=0.100, error=197.040\n",
            ">epoch=75, lrate=0.100, error=196.687\n",
            ">epoch=76, lrate=0.100, error=196.370\n",
            ">epoch=77, lrate=0.100, error=196.083\n",
            ">epoch=78, lrate=0.100, error=195.823\n",
            ">epoch=79, lrate=0.100, error=195.585\n",
            ">epoch=80, lrate=0.100, error=195.367\n",
            ">epoch=81, lrate=0.100, error=195.167\n",
            ">epoch=82, lrate=0.100, error=194.981\n",
            ">epoch=83, lrate=0.100, error=194.809\n",
            ">epoch=84, lrate=0.100, error=194.648\n",
            ">epoch=85, lrate=0.100, error=194.498\n",
            ">epoch=86, lrate=0.100, error=194.356\n",
            ">epoch=87, lrate=0.100, error=194.222\n",
            ">epoch=88, lrate=0.100, error=194.095\n",
            ">epoch=89, lrate=0.100, error=193.973\n",
            ">epoch=90, lrate=0.100, error=193.858\n",
            ">epoch=91, lrate=0.100, error=193.747\n",
            ">epoch=92, lrate=0.100, error=193.641\n",
            ">epoch=93, lrate=0.100, error=193.538\n",
            ">epoch=94, lrate=0.100, error=193.439\n",
            ">epoch=95, lrate=0.100, error=193.343\n",
            ">epoch=96, lrate=0.100, error=193.251\n",
            ">epoch=97, lrate=0.100, error=193.161\n",
            ">epoch=98, lrate=0.100, error=193.074\n",
            ">epoch=99, lrate=0.100, error=192.989\n",
            "8\n",
            "2\n",
            ">epoch=0, lrate=0.100, error=311.529\n",
            ">epoch=1, lrate=0.100, error=287.080\n",
            ">epoch=2, lrate=0.100, error=287.041\n",
            ">epoch=3, lrate=0.100, error=287.002\n",
            ">epoch=4, lrate=0.100, error=286.963\n",
            ">epoch=5, lrate=0.100, error=286.922\n",
            ">epoch=6, lrate=0.100, error=286.881\n",
            ">epoch=7, lrate=0.100, error=286.840\n",
            ">epoch=8, lrate=0.100, error=286.797\n",
            ">epoch=9, lrate=0.100, error=286.754\n",
            ">epoch=10, lrate=0.100, error=286.709\n",
            ">epoch=11, lrate=0.100, error=286.664\n",
            ">epoch=12, lrate=0.100, error=286.617\n",
            ">epoch=13, lrate=0.100, error=286.570\n",
            ">epoch=14, lrate=0.100, error=286.521\n",
            ">epoch=15, lrate=0.100, error=286.471\n",
            ">epoch=16, lrate=0.100, error=286.420\n",
            ">epoch=17, lrate=0.100, error=286.367\n",
            ">epoch=18, lrate=0.100, error=286.313\n",
            ">epoch=19, lrate=0.100, error=286.257\n",
            ">epoch=20, lrate=0.100, error=286.200\n",
            ">epoch=21, lrate=0.100, error=286.142\n",
            ">epoch=22, lrate=0.100, error=286.081\n",
            ">epoch=23, lrate=0.100, error=286.019\n",
            ">epoch=24, lrate=0.100, error=285.955\n",
            ">epoch=25, lrate=0.100, error=285.890\n",
            ">epoch=26, lrate=0.100, error=285.822\n",
            ">epoch=27, lrate=0.100, error=285.753\n",
            ">epoch=28, lrate=0.100, error=285.682\n",
            ">epoch=29, lrate=0.100, error=285.609\n",
            ">epoch=30, lrate=0.100, error=285.533\n",
            ">epoch=31, lrate=0.100, error=285.456\n",
            ">epoch=32, lrate=0.100, error=285.377\n",
            ">epoch=33, lrate=0.100, error=285.296\n",
            ">epoch=34, lrate=0.100, error=285.213\n",
            ">epoch=35, lrate=0.100, error=285.127\n",
            ">epoch=36, lrate=0.100, error=285.039\n",
            ">epoch=37, lrate=0.100, error=284.949\n",
            ">epoch=38, lrate=0.100, error=284.857\n",
            ">epoch=39, lrate=0.100, error=284.762\n",
            ">epoch=40, lrate=0.100, error=284.663\n",
            ">epoch=41, lrate=0.100, error=284.562\n",
            ">epoch=42, lrate=0.100, error=284.457\n",
            ">epoch=43, lrate=0.100, error=284.347\n",
            ">epoch=44, lrate=0.100, error=284.233\n",
            ">epoch=45, lrate=0.100, error=284.112\n",
            ">epoch=46, lrate=0.100, error=283.985\n",
            ">epoch=47, lrate=0.100, error=283.850\n",
            ">epoch=48, lrate=0.100, error=283.706\n",
            ">epoch=49, lrate=0.100, error=283.550\n",
            ">epoch=50, lrate=0.100, error=283.380\n",
            ">epoch=51, lrate=0.100, error=283.193\n",
            ">epoch=52, lrate=0.100, error=282.986\n",
            ">epoch=53, lrate=0.100, error=282.753\n",
            ">epoch=54, lrate=0.100, error=282.488\n",
            ">epoch=55, lrate=0.100, error=282.183\n",
            ">epoch=56, lrate=0.100, error=281.828\n",
            ">epoch=57, lrate=0.100, error=281.407\n",
            ">epoch=58, lrate=0.100, error=280.901\n",
            ">epoch=59, lrate=0.100, error=280.282\n",
            ">epoch=60, lrate=0.100, error=279.515\n",
            ">epoch=61, lrate=0.100, error=278.547\n",
            ">epoch=62, lrate=0.100, error=277.309\n",
            ">epoch=63, lrate=0.100, error=275.706\n",
            ">epoch=64, lrate=0.100, error=273.615\n",
            ">epoch=65, lrate=0.100, error=270.885\n",
            ">epoch=66, lrate=0.100, error=267.348\n",
            ">epoch=67, lrate=0.100, error=262.851\n",
            ">epoch=68, lrate=0.100, error=257.323\n",
            ">epoch=69, lrate=0.100, error=250.867\n",
            ">epoch=70, lrate=0.100, error=243.818\n",
            ">epoch=71, lrate=0.100, error=236.673\n",
            ">epoch=72, lrate=0.100, error=229.904\n",
            ">epoch=73, lrate=0.100, error=223.812\n",
            ">epoch=74, lrate=0.100, error=218.508\n",
            ">epoch=75, lrate=0.100, error=213.978\n",
            ">epoch=76, lrate=0.100, error=210.156\n",
            ">epoch=77, lrate=0.100, error=206.957\n",
            ">epoch=78, lrate=0.100, error=204.295\n",
            ">epoch=79, lrate=0.100, error=202.091\n",
            ">epoch=80, lrate=0.100, error=200.273\n",
            ">epoch=81, lrate=0.100, error=198.779\n",
            ">epoch=82, lrate=0.100, error=197.553\n",
            ">epoch=83, lrate=0.100, error=196.547\n",
            ">epoch=84, lrate=0.100, error=195.719\n",
            ">epoch=85, lrate=0.100, error=195.037\n",
            ">epoch=86, lrate=0.100, error=194.473\n",
            ">epoch=87, lrate=0.100, error=194.005\n",
            ">epoch=88, lrate=0.100, error=193.614\n",
            ">epoch=89, lrate=0.100, error=193.286\n",
            ">epoch=90, lrate=0.100, error=193.009\n",
            ">epoch=91, lrate=0.100, error=192.773\n",
            ">epoch=92, lrate=0.100, error=192.569\n",
            ">epoch=93, lrate=0.100, error=192.391\n",
            ">epoch=94, lrate=0.100, error=192.234\n",
            ">epoch=95, lrate=0.100, error=192.094\n",
            ">epoch=96, lrate=0.100, error=191.966\n",
            ">epoch=97, lrate=0.100, error=191.850\n",
            ">epoch=98, lrate=0.100, error=191.742\n",
            ">epoch=99, lrate=0.100, error=191.642\n",
            "8\n",
            "2\n",
            "Scores: [74.50980392156863, 78.43137254901961, 79.73856209150327, 75.81699346405229, 73.8562091503268]\n",
            "Mean Accuracy: 76.471%\n"
          ]
        }
      ],
      "source": [
        "# Test Backprop on Seeds dataset\n",
        "seed(1)\n",
        "\n",
        "# Convert the \"outcome\" to int  \n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./support-vector-machine-example-1.csv\")\n",
        "df.head(10)\n",
        "dataset = df.values.tolist()\n",
        "\n",
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.1\n",
        "n_epoch = 100\n",
        "n_hidden_1 = 3\n",
        "n_hidden_2 = 5 \n",
        "scores, trained_network = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden_1,n_hidden_2)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZbCSMCPRtZ5"
      },
      "source": [
        "After training, we can make use of the model to predict whether a patient get diabetes(i.e. a probability between 0 and 1, with 1 is getting diabetes, while 0 is not getting diabetes)\n",
        "\n",
        "I just randomly create some numbers and test whether the person would have chance to get diabetes. \n",
        "\n",
        "(The last entry of the the test_row is outcome, but it does not mean the outcome is 1. **It is just created for the sake of program design**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNtk_LHWSd9K",
        "outputId": "c4354154-6cb7-4d30-d9c8-638d9bdfcb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[16, 80, 69, 36, 70, 9.97249206121743, 1.028591683917503, 60, 1]\n",
            "The result of random_test_row is 1\n"
          ]
        }
      ],
      "source": [
        "# row has a structure like the following\n",
        "# [Pregnacies, glucose, blood pressure, skin thickness, Insulin, BMI, DiabetesPedigreeFunction, Age, (Outcome)]\n",
        "import random\n",
        "random_test_row = [random.randint(0,30),\n",
        "                   random.randint(0,150),\n",
        "                   random.randint(0,96),\n",
        "                   random.randint(0,40),\n",
        "                   random.randint(0,100),\n",
        "                   random.uniform(0,35.3),\n",
        "                   random.uniform(0,2.5),\n",
        "                   random.randint(0,60),\n",
        "                 random.choice([0,1])]\n",
        "\n",
        "predicted_prob = predict(trained_network, random_test_row)\n",
        "print(random_test_row)\n",
        "print(\"The result of random_test_row is \" + str(predicted_prob))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PHYS3151_Assignment_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
